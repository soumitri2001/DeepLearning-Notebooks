{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_for_dummies_part 1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "h8SBEBVtMZsP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcIW4bbpoPEb"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8SBEBVtMZsP"
      },
      "source": [
        "### Elementary stuffs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_9VoN3son3t",
        "outputId": "37219da7-a6d0-40d0-f12d-e259a11c2a00"
      },
      "source": [
        "# tensors\r\n",
        "\r\n",
        "x = torch.ones([2,2,3],dtype=torch.float16)\r\n",
        "\r\n",
        "print(x.size())\r\n",
        "\r\n",
        "# get tensor from a list\r\n",
        "arr = [2,3,4]\r\n",
        "x = torch.tensor(arr)\r\n",
        "\r\n",
        "# add and basic ops\r\n",
        "A = torch.randn([2,2])\r\n",
        "B = torch.randn([2,2])\r\n",
        "\r\n",
        "C = A + B\r\n",
        "print(C) \r\n",
        "\r\n",
        "C = torch.add(A,B) # same as A+B\r\n",
        "print(C)\r\n",
        "\r\n",
        "C.sub_(C) # in-place: x -= y \r\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 3])\n",
            "tensor([[ 0.2677,  0.1813],\n",
            "        [-0.0881, -0.0755]])\n",
            "tensor([[ 0.2677,  0.1813],\n",
            "        [-0.0881, -0.0755]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgq7qi0FoyQQ",
        "outputId": "b190896f-d83d-444e-f3b7-a4145296d28f"
      },
      "source": [
        "# slicing ops\r\n",
        "\r\n",
        "A = torch.tensor([[1,2,3],\r\n",
        "                  [4,5,6]])\r\n",
        "\r\n",
        "print(A[:,1]) # all rows of 2nd column\r\n",
        "print(A[0,:]) # all columns of 1st row"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 5])\n",
            "tensor([1, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU_E_qgqqWfR",
        "outputId": "2d7e36d4-a360-4658-d60c-f94e26dc81d4"
      },
      "source": [
        "# reshaping  tensors: view() method\r\n",
        "\r\n",
        "x = torch.randn(4,4)\r\n",
        "print(x)\r\n",
        "\r\n",
        "y = x.view(16) # reshaping to a Nx1 tensor\r\n",
        "print(y)\r\n",
        "\r\n",
        "y = x.view(-1,2) # specifying the column dimension only: 2x8\r\n",
        "print(y)\r\n",
        "\r\n",
        "y = x.view(-1,16) # 1x16\r\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1400,  0.4221,  1.1614, -0.1205],\n",
            "        [-0.4425, -0.5927, -0.5093, -1.6669],\n",
            "        [-0.0986,  0.2028, -0.9924,  0.2785],\n",
            "        [ 0.4964,  0.2540,  0.3547, -0.4598]])\n",
            "tensor([ 0.1400,  0.4221,  1.1614, -0.1205, -0.4425, -0.5927, -0.5093, -1.6669,\n",
            "        -0.0986,  0.2028, -0.9924,  0.2785,  0.4964,  0.2540,  0.3547, -0.4598])\n",
            "tensor([[ 0.1400,  0.4221],\n",
            "        [ 1.1614, -0.1205],\n",
            "        [-0.4425, -0.5927],\n",
            "        [-0.5093, -1.6669],\n",
            "        [-0.0986,  0.2028],\n",
            "        [-0.9924,  0.2785],\n",
            "        [ 0.4964,  0.2540],\n",
            "        [ 0.3547, -0.4598]])\n",
            "tensor([[ 0.1400,  0.4221,  1.1614, -0.1205, -0.4425, -0.5927, -0.5093, -1.6669,\n",
            "         -0.0986,  0.2028, -0.9924,  0.2785,  0.4964,  0.2540,  0.3547, -0.4598]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKYjaGewrSQH",
        "outputId": "96284e93-60ad-42c1-c40f-5cd52f3b623c"
      },
      "source": [
        "# convert tensor to numpy array and vice-versa\r\n",
        "\r\n",
        "x = torch.randn(4)\r\n",
        "print(type(x))\r\n",
        "\r\n",
        "y = x.numpy() # convert tensor to numpy\r\n",
        "\r\n",
        "print(type(y))\r\n",
        "\r\n",
        "z = torch.from_numpy(y)\r\n",
        "print(type(z))\r\n",
        "\r\n",
        "'''\r\n",
        "if tensor is on cpu, then even after conversion both the tensor \r\n",
        "and the numpy array will point to the same memory location,\r\n",
        "so changes in any of them will change values of both.\r\n",
        "\r\n",
        "to declare a tensor on gpu:\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "    \r\n",
        "    device = torch.device('cuda')\r\n",
        "    x = torch.ones(5,device=device)\r\n",
        "    y = torch.ones(5)\r\n",
        "    y = y.to(device) # this moves tensor to the specified device\r\n",
        "\r\n",
        "    # numpy can only handle cpu tensors, so to convert a gpu \r\n",
        "    # tensor back to numpy we need to first take it to the cpu.\r\n",
        "\r\n",
        "    z = x + y\r\n",
        "    z = z.to('cpu')  \r\n",
        "\r\n",
        "'''\r\n",
        "\r\n",
        "A = np.ones(5)\r\n",
        "B = torch.from_numpy(A)\r\n",
        "\r\n",
        "print(A)\r\n",
        "print(B)\r\n",
        "\r\n",
        "A += 1 # this changes values in B also\r\n",
        "\r\n",
        "print(A)\r\n",
        "print(B)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'torch.Tensor'>\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaBY8FxTuY85",
        "outputId": "8fbec4e0-62c0-47f0-e25f-2fea5a515f19"
      },
      "source": [
        "# calculating gradients using autograd\r\n",
        "\r\n",
        "x = torch.randn(3,requires_grad=True) # this parameter allows us to \r\n",
        "                                      # calculate gradients wrt this tensor \r\n",
        "                                      # by creating computational graphs\r\n",
        "print(x)\r\n",
        "\r\n",
        "y = 2*x-1\r\n",
        "print(y)\r\n",
        "\r\n",
        "z = y*y*2\r\n",
        "print(z)\r\n",
        "\r\n",
        "z = z.mean()\r\n",
        "print(z)\r\n",
        "\r\n",
        "# backpropagation\r\n",
        "z.backward() # dz/dx: note that z needs to be a scalar\r\n",
        "print(x.grad) # gradients using chain rule\r\n",
        "\r\n",
        "'''\r\n",
        "if we need to update weights in training loop - these Ops\r\n",
        "should not be part of gradient calculation\r\n",
        "\r\n",
        "the following 3 ways prevents tracking of history of gradients:\r\n",
        "    \r\n",
        "    x.requires_grad_(False) - inplace change\r\n",
        "    x.detach() - creates a new tensor with same values\r\n",
        "    wrap it in a \"with torch.no_grad():\" statement\r\n",
        "'''\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    y = x + 2\r\n",
        "    print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.4986,  0.8017, -0.2202], requires_grad=True)\n",
            "tensor([-1.9973,  0.6033, -1.4405], grad_fn=<SubBackward0>)\n",
            "tensor([7.9783, 0.7280, 4.1500], grad_fn=<MulBackward0>)\n",
            "tensor(4.2854, grad_fn=<MeanBackward0>)\n",
            "tensor([-5.3261,  1.6088, -3.8413])\n",
            "tensor([1.5014, 2.8017, 1.7798])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kou2hFMyuc21",
        "outputId": "548bb4f7-bcc1-439d-802a-2bec80dacc44"
      },
      "source": [
        "weights= torch.ones(4,requires_grad=True)\r\n",
        "\r\n",
        "for epoch in range(1):\r\n",
        "    \r\n",
        "    output = (weights*3).sum()\r\n",
        "    \r\n",
        "    output.backward() # calculating gradients\r\n",
        "    \r\n",
        "    print(weights.grad)\r\n",
        "\r\n",
        "    weights.grad.zero_() # emptying gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMbLeFdAMDkd"
      },
      "source": [
        "### linear regression - manual + using autograd for gradient computation\r\n",
        "\r\n",
        "##### f(x) = 2x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkqeTxae-uRX",
        "outputId": "71b9027f-f7eb-4aa9-a346-3f331d38d0e8"
      },
      "source": [
        "# linear regression manually using numpy only \r\n",
        "\r\n",
        "X = np.array([1,2,3,4],dtype=np.float32)\r\n",
        "y = np.array([2,4,6,8],dtype=np.float32)\r\n",
        "\r\n",
        "w = 0.0\r\n",
        "\r\n",
        "# model prediction\r\n",
        "def forward(x):\r\n",
        "    return w*x\r\n",
        "\r\n",
        "# loss = MSE\r\n",
        "def loss(y,y_pred):\r\n",
        "    return ((y_pred-y)**2).mean()\r\n",
        "\r\n",
        "# gradient\r\n",
        "# MSE = (1/N) *( w*x - y)**2\r\n",
        "# dJ/dw = (1/N) * dot(2*x,(w*x - y))\r\n",
        "def gradient(x,y,y_pred):\r\n",
        "    return np.dot(2*x,y_pred-y).mean()\r\n",
        "\r\n",
        "\r\n",
        "print(f'prediction before training: f(5) = {forward(5):.3f}')\r\n",
        "\r\n",
        "# training:\r\n",
        "learning_rate = 0.01\r\n",
        "n_epochs = 10\r\n",
        "\r\n",
        "for epoch in range(n_epochs):\r\n",
        "    \r\n",
        "    # preds = = forward pass\r\n",
        "    y_pred = forward(X)\r\n",
        "\r\n",
        "    # loss\r\n",
        "    l = loss(y,y_pred)\r\n",
        "\r\n",
        "    # gradients\r\n",
        "    dw = gradient(X,y,y_pred)\r\n",
        "\r\n",
        "    # update weights\r\n",
        "    w = w - learning_rate * dw\r\n",
        "\r\n",
        "    print(f'epoch {(epoch+1)*1}: w = {w:.3f},loss = {l:.8f}')\r\n",
        "\r\n",
        "print(f'prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200,loss = 30.00000000\n",
            "epoch 2: w = 1.680,loss = 4.79999924\n",
            "epoch 3: w = 1.872,loss = 0.76800019\n",
            "epoch 4: w = 1.949,loss = 0.12288000\n",
            "epoch 5: w = 1.980,loss = 0.01966083\n",
            "epoch 6: w = 1.992,loss = 0.00314574\n",
            "epoch 7: w = 1.997,loss = 0.00050331\n",
            "epoch 8: w = 1.999,loss = 0.00008053\n",
            "epoch 9: w = 1.999,loss = 0.00001288\n",
            "epoch 10: w = 2.000,loss = 0.00000206\n",
            "prediction after training: f(5) = 9.999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_yeGNydD-uR",
        "outputId": "3ba432e2-bea2-4d1a-df4d-d62467b6b1a4"
      },
      "source": [
        "# linear regression using a Linear layer and autograd for gradient computation\r\n",
        "\r\n",
        "'''\r\n",
        "pipeline:\r\n",
        "\r\n",
        "1) design model (input_size,output_size,forward_pass)\r\n",
        "2) construct loss and optimizer\r\n",
        "3) training loop over n_epochs:\r\n",
        "    - forward pass: compute predictions\r\n",
        "    - backward pass: compute gradients\r\n",
        "    - update weights\r\n",
        "\r\n",
        "'''\r\n",
        "\r\n",
        "# data for training\r\n",
        "X = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\r\n",
        "y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\r\n",
        "\r\n",
        "# for testing\r\n",
        "X_test = torch.tensor([5],dtype=torch.float32) # to be passed for testing model\r\n",
        "\r\n",
        "n_samples,n_features = X.shape\r\n",
        "print(n_samples,n_features)\r\n",
        "\r\n",
        "# w = torch.tensor(0.0,\r\n",
        "#                  dtype=torch.float32,\r\n",
        "#                  requires_grad=True)\r\n",
        "\r\n",
        "# model declaration: we need input_size,output_size\r\n",
        "input_size = n_features\r\n",
        "output_size = n_features\r\n",
        "\r\n",
        "# creating a custom NN model i.e. class inheriting nn.Module:\r\n",
        "\r\n",
        "class LinearRegression(nn.Module):\r\n",
        "    def __init__(self,input_size,output_size):\r\n",
        "        super(LinearRegression,self).__init__()\r\n",
        "        #define layers\r\n",
        "        self.Linear1 = nn.Linear(input_size,output_size)\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        return self.Linear1(x)\r\n",
        "\r\n",
        "### model = nn.Linear(input_size,output_size) # single FC layer\r\n",
        "\r\n",
        "model = LinearRegression(input_size,output_size)\r\n",
        "\r\n",
        "print(f'prediction before training: f(5) = {model(X_test).item():.3f}') # passing a tensor to the model\r\n",
        "\r\n",
        "# training:\r\n",
        "learning_rate = 0.02\r\n",
        "n_epochs = 200\r\n",
        "\r\n",
        "# constructing loss (i.e. criterion) and optimizer\r\n",
        "\r\n",
        "criterion = nn.MSELoss() # MSE loss, for LinReg\r\n",
        "optimizer = torch.optim.SGD(params=model.parameters(),\r\n",
        "                            lr=learning_rate) # stochastic gradient descent\r\n",
        "\r\n",
        "for epoch in range(n_epochs):\r\n",
        "    # getting the model parameters\r\n",
        "    [w,b] = model.parameters()\r\n",
        "\r\n",
        "    # preds = = forward pass\r\n",
        "    y_pred = forward(X)\r\n",
        "\r\n",
        "    # loss - using defined criterion \r\n",
        "    loss = criterion(y_pred,y) \r\n",
        "\r\n",
        "    # gradients: using backward()\r\n",
        "    loss.backward() # dl/dw\r\n",
        "\r\n",
        "    # update weights: the gradients should not be updated ### w -= learning_rate * loss.grad\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # make gradients = 0 ### w.grad.zero_()\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    if epoch % 10 == 0:\r\n",
        "        print(f'epoch {epoch*1}: w = {w[0][0].item():.3f},loss = {l:.8f}')\r\n",
        " \r\n",
        "print(f'prediction after training: f(5) = {model(X_test).item():.3f}') # passing tensor to model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 1\n",
            "prediction before training: f(5) = 2.018\n",
            "epoch 0: w = 0.782,loss = 0.00000363\n",
            "epoch 10: w = 1.966,loss = 0.00000363\n",
            "epoch 20: w = 1.999,loss = 0.00000363\n",
            "epoch 30: w = 2.000,loss = 0.00000363\n",
            "epoch 40: w = 2.000,loss = 0.00000363\n",
            "epoch 50: w = 2.000,loss = 0.00000363\n",
            "epoch 60: w = 2.000,loss = 0.00000363\n",
            "epoch 70: w = 2.000,loss = 0.00000363\n",
            "epoch 80: w = 2.000,loss = 0.00000363\n",
            "epoch 90: w = 2.000,loss = 0.00000363\n",
            "epoch 100: w = 2.000,loss = 0.00000363\n",
            "epoch 110: w = 2.000,loss = 0.00000363\n",
            "epoch 120: w = 2.000,loss = 0.00000363\n",
            "epoch 130: w = 2.000,loss = 0.00000363\n",
            "epoch 140: w = 2.000,loss = 0.00000363\n",
            "epoch 150: w = 2.000,loss = 0.00000363\n",
            "epoch 160: w = 2.000,loss = 0.00000363\n",
            "epoch 170: w = 2.000,loss = 0.00000363\n",
            "epoch 180: w = 2.000,loss = 0.00000363\n",
            "epoch 190: w = 2.000,loss = 0.00000363\n",
            "prediction after training: f(5) = 10.721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUjTHpokWzM5"
      },
      "source": [
        "### Linear Regression practical using Pytorch and standard dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "IkehH2iQRWVD",
        "outputId": "ccba0510-fc15-427b-a6f3-f1e080a302ca"
      },
      "source": [
        "'''\r\n",
        "pipeline:\r\n",
        "\r\n",
        "1) design model (input_size,output_size,forward_pass)\r\n",
        "2) construct loss and optimizer\r\n",
        "3) training loop over n_epochs:\r\n",
        "    - forward pass: compute predictions\r\n",
        "    - backward pass: compute gradients\r\n",
        "    - update weights\r\n",
        "\r\n",
        "'''\r\n",
        "\r\n",
        "from sklearn import datasets\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# preparing dataset\r\n",
        "X_npy, y_npy = datasets.make_regression(n_samples=100,\r\n",
        "                                        n_features=1,\r\n",
        "                                        noise=20,\r\n",
        "                                        random_state=1)\r\n",
        "\r\n",
        "# convert to tensor\r\n",
        "X = torch.from_numpy(X_npy.astype(np.float32))\r\n",
        "y = torch.from_numpy(y_npy.astype(np.float32))\r\n",
        "\r\n",
        "y = y.view(y.shape[0],1) # reshape to column tensor\r\n",
        "\r\n",
        "n_samples, n_features = X.shape\r\n",
        "print(f'Nos of samples = {n_samples} \\n Nos of features = {n_features}')\r\n",
        "\r\n",
        "# 1) model declaration\r\n",
        "input_size = n_features\r\n",
        "output_size = 1\r\n",
        "\r\n",
        "model = nn.Linear(input_size,output_size) \r\n",
        "\r\n",
        "# 2) loss and optimizer\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\r\n",
        "\r\n",
        "# 3) training loop\r\n",
        "n_epochs = 100\r\n",
        "\r\n",
        "for epoch in range(n_epochs):\r\n",
        "    # (i) forward pass and loss\r\n",
        "    y_predicted = model(X)\r\n",
        "    loss = criterion(y_predicted,y)\r\n",
        "\r\n",
        "    # (ii) backward pass - gradients\r\n",
        "    loss.backward()\r\n",
        "    \r\n",
        "    # (iii) update\r\n",
        "    optimizer.step()\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    if epoch % 10 == 0:\r\n",
        "        print(f'Epoch: {epoch},loss = {loss.item():.6f}')\r\n",
        "\r\n",
        "\r\n",
        "predicted = model(X).detach().numpy() # removing from computation graph s.t. requires_grads=False\r\n",
        "\r\n",
        "# plotting predictions\r\n",
        "plt.plot(X_npy,y_npy,'ro')\r\n",
        "plt.plot(X_npy,predicted,'b')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nos of samples = 100 \n",
            " Nos of features = 1\n",
            "Epoch: 0,loss = 5876.020508\n",
            "Epoch: 10,loss = 4349.300781\n",
            "Epoch: 20,loss = 3245.851562\n",
            "Epoch: 30,loss = 2447.410889\n",
            "Epoch: 40,loss = 1869.056885\n",
            "Epoch: 50,loss = 1449.708740\n",
            "Epoch: 60,loss = 1145.375122\n",
            "Epoch: 70,loss = 924.325134\n",
            "Epoch: 80,loss = 763.643066\n",
            "Epoch: 90,loss = 646.759583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjtHiNy5asr5"
      },
      "source": [
        "### Binary classification practical using Pytorch and standard scikit dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhPbN_2rRWTp",
        "outputId": "b125020d-c354-4bb7-810c-3d9528c90e3e"
      },
      "source": [
        "# similar to LinearRegression, only changes to be made in model and criterion\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# prepare data - load preloaded dataset from scikit-learn\r\n",
        "dataset = datasets.load_breast_cancer()\r\n",
        "\r\n",
        "X,y = dataset.data, dataset.target # numpy arrays\r\n",
        "\r\n",
        "n_samples, n_features = X.shape\r\n",
        "\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\r\n",
        "\r\n",
        "# normalization (preprocessing data)\r\n",
        "sc = StandardScaler()\r\n",
        "X_train = sc.fit_transform(X_train)\r\n",
        "X_test = sc.fit_transform(X_test)\r\n",
        "\r\n",
        "# convert to tensors\r\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\r\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\r\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\r\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\r\n",
        "\r\n",
        "# reshape target labels to a column tensor\r\n",
        "y_train = y_train.view(y_train.shape[0],1)\r\n",
        "y_test = y_test.view(y_test.shape[0],1)\r\n",
        "\r\n",
        "# 1) define model\r\n",
        "class Classifier(nn.Module):\r\n",
        "    def __init__(self,n_input_features):\r\n",
        "        super(Classifier,self).__init__()\r\n",
        "        self.Linear = nn.Linear(n_input_features,1)\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        y_predicted = torch.sigmoid(self.Linear(x))\r\n",
        "        return y_predicted\r\n",
        "\r\n",
        "model = Classifier(n_features)\r\n",
        "\r\n",
        "# 2) criterion and optimizer\r\n",
        "criterion = nn.BCELoss() # binary_crossentropy_loss\r\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\r\n",
        "\r\n",
        "# 3) training loop\r\n",
        "num_epochs = 500\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    # (i) forward pass and loss\r\n",
        "    y_predicted = model(X_train)\r\n",
        "    loss = criterion(y_predicted,y_train)\r\n",
        "\r\n",
        "    # (ii) backward pass\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # (iii) update parameters\r\n",
        "    optimizer.step()\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    if (epoch+1) % 10 == 0:\r\n",
        "        print(f'Epoch: {epoch+1}, loss = {loss.item():.6f}')\r\n",
        "\r\n",
        "\r\n",
        "# evaluation of model - need not track gradient history, hence torch.no_grad() is used\r\n",
        "with torch.no_grad():\r\n",
        "    y_predicted = model(X_test)\r\n",
        "    y_predicted_cls = y_predicted.round() # >0.5 => 1 else 0\r\n",
        "    correct = y_predicted_cls.eq(y_test).sum() # each correct pred => adds 1\r\n",
        "    accuracy = correct/float(y_test.shape[0]) \r\n",
        "    print(f'Accuracy on test data = {accuracy:.4f}')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, loss = 0.436395\n",
            "Epoch: 20, loss = 0.263768\n",
            "Epoch: 30, loss = 0.196911\n",
            "Epoch: 40, loss = 0.162123\n",
            "Epoch: 50, loss = 0.141761\n",
            "Epoch: 60, loss = 0.128666\n",
            "Epoch: 70, loss = 0.119357\n",
            "Epoch: 80, loss = 0.112245\n",
            "Epoch: 90, loss = 0.106558\n",
            "Epoch: 100, loss = 0.101854\n",
            "Epoch: 110, loss = 0.097867\n",
            "Epoch: 120, loss = 0.094427\n",
            "Epoch: 130, loss = 0.091420\n",
            "Epoch: 140, loss = 0.088765\n",
            "Epoch: 150, loss = 0.086399\n",
            "Epoch: 160, loss = 0.084275\n",
            "Epoch: 170, loss = 0.082356\n",
            "Epoch: 180, loss = 0.080611\n",
            "Epoch: 190, loss = 0.079017\n",
            "Epoch: 200, loss = 0.077555\n",
            "Epoch: 210, loss = 0.076207\n",
            "Epoch: 220, loss = 0.074961\n",
            "Epoch: 230, loss = 0.073804\n",
            "Epoch: 240, loss = 0.072727\n",
            "Epoch: 250, loss = 0.071722\n",
            "Epoch: 260, loss = 0.070781\n",
            "Epoch: 270, loss = 0.069898\n",
            "Epoch: 280, loss = 0.069068\n",
            "Epoch: 290, loss = 0.068286\n",
            "Epoch: 300, loss = 0.067548\n",
            "Epoch: 310, loss = 0.066849\n",
            "Epoch: 320, loss = 0.066187\n",
            "Epoch: 330, loss = 0.065559\n",
            "Epoch: 340, loss = 0.064962\n",
            "Epoch: 350, loss = 0.064393\n",
            "Epoch: 360, loss = 0.063851\n",
            "Epoch: 370, loss = 0.063333\n",
            "Epoch: 380, loss = 0.062838\n",
            "Epoch: 390, loss = 0.062364\n",
            "Epoch: 400, loss = 0.061910\n",
            "Epoch: 410, loss = 0.061474\n",
            "Epoch: 420, loss = 0.061055\n",
            "Epoch: 430, loss = 0.060653\n",
            "Epoch: 440, loss = 0.060265\n",
            "Epoch: 450, loss = 0.059891\n",
            "Epoch: 460, loss = 0.059530\n",
            "Epoch: 470, loss = 0.059182\n",
            "Epoch: 480, loss = 0.058845\n",
            "Epoch: 490, loss = 0.058520\n",
            "Epoch: 500, loss = 0.058204\n",
            "Accuracy on test data = 0.9737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFsVe-glhrxP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}