# -*- coding: utf-8 -*-
"""Breast-Histology(Kaggle dataset).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dnqL6SfMQLL8C_q7khY6E0ovzOQAFTz-
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# pytorch imports

import torch
import torch.nn as nn
import torchvision
from torch.utils.data import Dataset,DataLoader
import torchvision.transforms as transforms
import torch.nn.functional as F

DIR_PATH = '/content/gdrive/MyDrive/Kaggle/breast-histology'
data = np.load(os.path.join(DIR_PATH,'X.npy'))
target = np.load(os.path.join(DIR_PATH,'Y.npy'))

type(data),type(target)

data.shape,target.shape

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(data,target,test_size=0.1,random_state=2)

X_train.shape,y_train.shape,X_test.shape,y_test.shape

# plot some images
plt.figure(figsize = (16, 4))
for num, x in enumerate(X_train[0:6]):
    plt.subplot(1,6,num+1)
    plt.axis('off')
    plt.imshow(x)
    plt.title(y_train[num])

class DatasetProcessing(Dataset):
    
    '''
    The Dataloader reads the data and puts it into memory
    '''
    
    #initialise the class variables - transform, data, target
    def __init__(self, data, target, transform=None): 
        self.transform = transform
        self.data = data.astype(np.float32)[:,:,:,None]
        # converting target to torch.LongTensor dtype
        self.target = torch.from_numpy(target).long() 
    
    '''
    Data must be wrapped on a Dataset parent class where 
    the methods getitem and len must be overrided. 
    Note that, the data is not loaded on memory by now.
    '''
    
    #retrieve the X and y index value and return it
    def __getitem__(self, index): 
        return self.transform(self.data[index]), self.target[index]
    
    #returns the length of the data
    def __len__(self): 
        return len(list(self.data))

# image transformations
mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])

transformations = {    
    'train' : transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(224),
        # transforms.CenterCrop((224,224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        # transforms.RandomAffine(degrees=(-180,180), translate=(0.1,0.1), scale=(0.9,1.1), shear=(-5,5)),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ]),
    'test_val' : transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(224),
        # transforms.CenterCrop((224,224)),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
}

train_data = DatasetProcessing(X_train,y_train,transform=transformations['train'])
test_data = DatasetProcessing(X_test,y_test,transform=transformations['test_val'])

len(train_data),len(test_data)

classes,counts = np.unique(target,return_counts=True)

np.asarray((classes,counts)).T

# hyperparameters
train_batch_size = 2
learning_rate = 0.002
num_classes = 2
num_epochs = 30

device = None
if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

print(device)

train_loader = torch.utils.data.DataLoader(dataset=train_data,
                                           batch_size=train_batch_size,
                                           shuffle=True,
                                           num_workers=4)

test_loader = torch.utils.data.DataLoader(dataset=test_data,
                                          batch_size=1,
                                          shuffle=False,
                                          num_workers=4)

len(train_loader),len(test_loader)

examples = iter(train_loader)
samples,labels = examples.next()

print(samples.shape,labels.shape)

class ConvNet(nn.Module):
    def __init__(self,model,num_classes):
        super(ConvNet,self).__init__()
        self.base_model = nn.Sequential(*list(model.children())[:-1]) # model excluding last FC layer
        self.linear1 = nn.Linear(in_features=25088,out_features=512)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(in_features=512,out_features=num_classes)
    
    def forward(self,x):
        x = self.base_model(x)
        x = torch.flatten(x,1)
        lin = self.linear1(x)
        x = self.relu(lin)
        out = self.linear2(x)
        return lin, out

model = torchvision.models.vgg19_bn(pretrained=True)

model = ConvNet(model,num_classes)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)

# print(model)

def train_model(model,criterion,optimizer,dataloader,num_epochs=30):

    n_iters = len(dataloader)

    for epoch in range(num_epochs):
        
        model.train()

        for ii,(images,labels) in enumerate(dataloader):

            images = images.to(device)
            labels = labels.to(device)

            _,outputs = model(images)
            loss = criterion(outputs,labels)
                    
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if (ii+1)%100 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{ii+1}/{n_iters}], Loss = {loss.item():.6f}')
            
        print('-------------------------------------------------')

    return model

model = train_model(model,criterion,optimizer,train_loader,num_epochs=num_epochs)

